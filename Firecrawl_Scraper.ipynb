{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq3uvvsbaVkjKCSgzi8nEB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfjhf-l0Ctte"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# 1. ARRAY OF FIRECRAWL API KEYS\n",
        "# ===============================\n",
        "FIRECRAWL_KEYS = [\n",
        "    \"fc_key_1_here\",\n",
        "    \"fc_key_2_here\",\n",
        "    \"fc_key_3_here\"\n",
        "]\n",
        "\n",
        "# ====================\n",
        "# 2. ARRAY OF URLS\n",
        "# ====================\n",
        "URLS = [\n",
        "    \"https://example.com\",\n",
        "    \"https://openai.com\",\n",
        "    \"https://wikipedia.org\"\n",
        "]\n",
        "\n",
        "# ======================================\n",
        "# 3. REQUEST FUNCTION TO GET RAW HTML\n",
        "# ======================================\n",
        "def get_raw_html(url, api_key):\n",
        "    \"\"\"\n",
        "    Sends a Firecrawl scrape request and returns raw HTML.\n",
        "    \"\"\"\n",
        "    api_url = \"https://api.firecrawl.dev/v1/scrape\"\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"url\": url,\n",
        "        \"formats\": [\"rawHtml\"]\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            return response.json()[\"rawHtml\"]\n",
        "        except KeyError:\n",
        "            print(\"⚠️ No rawHtml returned:\", response.json())\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"❌ Error {response.status_code} for {url}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. FETCH + SAVE HTML FILES\n",
        "# ================================\n",
        "os.makedirs(\"html_raw\", exist_ok=True)\n",
        "\n",
        "for i, url in enumerate(URLS):\n",
        "    api_key = FIRECRAWL_KEYS[i % len(FIRECRAWL_KEYS)]  # rotate API key\n",
        "\n",
        "    print(f\"Fetching: {url} using key {api_key[:10]}...\")\n",
        "\n",
        "    html_content = get_raw_html(url, api_key)\n",
        "\n",
        "    if html_content:\n",
        "        filename = f\"html_raw/page_{i+1}.html\"\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html_content)\n",
        "        print(f\"✔ Saved: {filename}\")\n",
        "    else:\n",
        "        print(f\"❌ Failed to fetch: {url}\")\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 5. ZIP HTML FILES\n",
        "# ======================\n",
        "zip_filename = \"html-raw.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
        "    for file in os.listdir(\"html_raw\"):\n",
        "        filepath = os.path.join(\"html_raw\", file)\n",
        "        zipf.write(filepath, arcname=file)\n",
        "\n",
        "print(\"✔ Zipped into:\", zip_filename)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6. DOWNLOAD ZIP FILE\n",
        "# =========================\n",
        "files.download(zip_filename)\n",
        "print(\"⬇️ Download should begin automatically.\")"
      ]
    }
  ]
}