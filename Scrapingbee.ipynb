{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI9HtPWizEEEBh8q7toln7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb1oGuaVoH9W",
        "outputId": "d7e195f2-9a42-4b0e-c3ff-d8845023abca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total keys found: 4\n",
            "‚ùå Error: 403 {\"message\":\"Your account has been banned. Reason: Your email domain seems to abuse.\"}\n",
            "\n",
            "‚ùå Error: 401 {\"message\":\"Monthly API calls limit reached: 1000\"}\n",
            "\n",
            "‚ùå Error: 401 {\"message\":\"Monthly API calls limit reached: 1000\"}\n",
            "\n",
            "‚ùå Error: 401 {\"message\":\"Monthly API calls limit reached: 1000\"}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def validate_api_key(api_key):\n",
        "    TARGET_URL = \"https://books.toscrape.com/\"\n",
        "    SCRAPINGBEE_ENDPOINT = \"https://app.scrapingbee.com/api/v1/\"\n",
        "    params = {\n",
        "        \"api_key\": api_key,\n",
        "        \"url\": TARGET_URL,\n",
        "        \"render_js\": \"false\"\n",
        "    }\n",
        "    response = requests.get(SCRAPINGBEE_ENDPOINT, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"‚ùå Error:\", response.status_code, response.text)\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "myText = r\"\"\"\n",
        "Skip to content\n",
        "Navigation Menu\n",
        "Search GitHub:\n",
        "app.scrapingbee.com AND api_key = \" language:Python\n",
        "code Search Results ¬∑ app.scrapingbee.com AND api_key = \" language:Python\n",
        "Filter by\n",
        "Repositories\n",
        "Paths\n",
        "Advanced\n",
        "426 files\n",
        " (2 s)\n",
        "426 files\n",
        "\n",
        "\n",
        "ValeriaLock/api-web-scraping2 ¬∑ scrap_igp.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "    response = requests.get(\n",
        "        \"https://app.scrapingbee.com/api/v1/\",\n",
        "        params={\n",
        "            \"api_key\": \"BO2PPUTR0VEC1DVF7V49UYSE8BXO3A9JOD57X62ZN7VYC0X3XZ7EK7A0UVHRKBQ53OT3UD4SWK992GTO\",\n",
        "            \"url\": url_objetivo,\n",
        "            \"render_js\": \"true\",\n",
        "            \"wait\": \"5000\"  # Esperar 5 segundos para que cargue la tabla\n",
        "Show 19 more matches\n",
        "\n",
        "\n",
        "lilhat/CompareMySupplies ¬∑ webScraper_v2/toolstationScraper.py\n",
        "Python\n",
        "¬∑\n",
        "master\n",
        "def single_request(url):\n",
        "    for _ in range(NUM_RETRIES):\n",
        "        response = requests.get(\n",
        "            url='https://app.scrapingbee.com/api/v1/',\n",
        "            params={\n",
        "                'api_key': 'N25JJBPDKWXCENSFCR66CALWK0CE0QHEUE2H82Y2S1RYM4RQGHC1LTMTCX7DIONSJFYSP2ONBX2L0SRI',\n",
        "                'url': url,\n",
        "Show 78 more matches\n",
        "\n",
        "\n",
        "trilu/lupito-content ¬∑ phase1_quick_wins_assault.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "    def scrape_with_scrapingbee(self, url, breed_name=\"\"):\n",
        "        try:\n",
        "            response = requests.get('https://app.scrapingbee.com/api/v1/', params=params, timeout=30)\n",
        "Show 156 more matches\n",
        "\n",
        "\n",
        "Bakin-jpg/sankanime-scraper ¬∑ scraper.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "import time\n",
        "# Token API ScrapingBee\n",
        "API_KEY = os.getenv(\"SCRAPINGBEE_API_KEY\")\n",
        "# URL target episode anime\n",
        "url = \"https://sankanime.com/watch/nine-rulers-crown-19741\"\n",
        "Show 76 more matches\n",
        "\n",
        "\n",
        "abdul-28930/HMS-AI-V2-Backend ¬∑ services/scraping_service.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "class ScrapingService:\n",
        "    def __init__(self):\n",
        "        self.api_key = os.getenv(\"SCRAPINGBEE_API_KEY\")\n",
        "        self.base_url = \"https://app.scrapingbee.com/api/v1/\"\n",
        "\n",
        "    def get_competitor_prices(self, location: str):\n",
        "Show 16 more matches\n",
        "\n",
        "\n",
        "trilu/lupito-content ¬∑ scripts/scrape_zooplus_batch.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "load_dotenv()\n",
        "SCRAPINGBEE_API_KEY = os.getenv('SCRAPING_BEE')\n",
        "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = os.getenv(\"SUPABASE_SERVICE_KEY\")\n",
        "Show 139 more matches\n",
        "\n",
        "\n",
        "mateohysa/anything-scraper ¬∑ source/proxy_controller.py\n",
        "Python\n",
        "¬∑\n",
        "master\n",
        "r = requests.get('http://api.scraperapi.com?apikey={}&url={}'.format(SCRAPER_API_KEY, url))\n",
        "### SCRAPING BEE\n",
        "BEE_API_KEY = \"someapikey\"\n",
        "r = requests.get(\n",
        "    url = 'https://app.scrapingbee.com/api/v1',\n",
        "    params={\n",
        "Show 35 more matches\n",
        "\n",
        "\n",
        "jkoestner/folioflex ¬∑ folioflex/chatbot/scraper.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "                logger.error(\"probably flagged bot: returning None\")\n",
        "                html_content = \"<html><body><p>could not scrape</p></body></html>\"\n",
        "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "    response = requests.get(\n",
        "        url=\"https://app.scrapingbee.com/api/v1/\",\n",
        "        params={\n",
        "Show 165 more matches\n",
        "\n",
        "\n",
        "Frxnesvo/TickyBot ¬∑ GestioneNews/src/pygooglenews.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "        response = requests.get(\n",
        "            url=\"https://app.scrapingbee.com/api/v1/\",\n",
        "            params={\n",
        "                \"api_key\": api_key,\n",
        "                \"url\": url,\n",
        "                \"render_js\": \"false\"\n",
        "            }\n",
        "Show 102 more matches\n",
        "\n",
        "\n",
        "trilu/lupito-content ¬∑ breed_standards_importer.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "    def scrape_with_scrapingbee(self, url, breed_name=\"\"):\n",
        "        try:\n",
        "            response = requests.get('https://app.scrapingbee.com/api/v1/',\n",
        "                                  params=params, timeout=45)\n",
        "Show 144 more matches\n",
        "\n",
        "\n",
        "trilu/lupito-content ¬∑ harvest_briantos.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "        params = {\n",
        "            'api_key': api_key,\n",
        "            'url': url,\n",
        "            response = requests.get(\n",
        "                'https://app.scrapingbee.com/api/v1/',\n",
        "                params=params,\n",
        "Show 126 more matches\n",
        "\n",
        "\n",
        "lilhat/CompareMySupplies ¬∑ web_scraper/bradfordsScraper.py\n",
        "Python\n",
        "¬∑\n",
        "master\n",
        "    for _ in range(NUM_RETRIES):\n",
        "        # Make GET request to ScrapingBee API\n",
        "        response = requests.get(\n",
        "            url='https://app.scrapingbee.com/api/v1/',\n",
        "            params={\n",
        "                'api_key': 'FKCMAMEUOONCRQ0WE8DQ58KA2SRPC1ZHN6V7JRPI2PMEK2380VBCQQPKALY3NQC83KMG5AQI2BGHPSRZ',\n",
        "                'url': url,\n",
        "Show 94 more matches\n",
        "\n",
        "\n",
        "trilu/lupito-content ¬∑ scrapingbee_harvester.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "        params = {\n",
        "            'api_key': self.api_key,\n",
        "            'url': url,\n",
        "            response = requests.get(\n",
        "                'https://app.scrapingbee.com/api/v1/',\n",
        "                params=params,\n",
        "Show 154 more matches\n",
        "\n",
        "\n",
        "ztay100/ztay100 ¬∑ live_scores.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "def fetch_page(url, api_key, debug_path, retries=5, timeout=60):\n",
        "    proxy_url = \"https://app.scrapingbee.com/api/v1/\"\n",
        "    params = {\n",
        "        \"api_key\": api_key,\n",
        "        \"url\": url,\n",
        "Show 119 more matches\n",
        "\n",
        "\n",
        "ramana22/job-watcher ¬∑ hiring_cafe_job_watcher.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "load_dotenv()\n",
        "SCRAPINGBEE_API_KEY = os.getenv(\"SCRAPINGBEE_API_KEY\")\n",
        "API_URL = \"https://hiring.cafe/api/search-jobs\"\n",
        "PROXY_URL = f\"https://app.scrapingbee.com/api/v1/?api_key={SCRAPINGBEE_API_KEY}&url=\"\n",
        "Show 160 more matches\n",
        "\n",
        "\n",
        "anujaomanthu/AI-RESEARCH-AGENT- ¬∑ app.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "# Replace with your actual ScrapingBee API key\n",
        "SCRAPINGBEE_API_KEY = os.getenv(\"SCRAPINGBEE_API_KEY\")\n",
        "serper_api_key = os.getenv(\"SERP_API_KEY\")\n",
        "# 1. Tool for search\n",
        "def search(query):\n",
        "Show 145 more matches\n",
        "\n",
        "\n",
        "Mohanad1206/scraper-final ¬∑ scraper/run.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "OUT_DIR = \"out\"\n",
        "OUT_JSONL = os.path.join(OUT_DIR, \"snapshot.jsonl\")\n",
        "OUT_CSV = os.path.join(OUT_DIR, \"snapshot.csv\")\n",
        "CONFIG_PATH = \"scraper/config.json\"\n",
        "SITES_PATH = \"scraper/sites.txt\"\n",
        "UA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
        "FIELDS = [\"timestamp_iso\",\"site_name\",\"product_name\",\"sku\",\"product_url\",\"status\",\"price_value\",\"currency\",\"raw_price_t‚Ä¶\n",
        "    s = re.sub(r'//.*', '', s)\n",
        "Show 156 more matches\n",
        "\n",
        "\n",
        "lilhat/CompareMySupplies ¬∑ web_scraper/toolstationScraper.py\n",
        "Python\n",
        "¬∑\n",
        "master\n",
        "    for _ in range(NUM_RETRIES):\n",
        "        # Make GET request to ScrapingBee API\n",
        "        response = requests.get(\n",
        "            url='https://app.scrapingbee.com/api/v1/',\n",
        "            params={\n",
        "                'api_key': 'FKCMAMEUOONCRQ0WE8DQ58KA2SRPC1ZHN6V7JRPI2PMEK2380VBCQQPKALY3NQC83KMG5AQI2BGHPSRZ',\n",
        "                'url': url,\n",
        "Show 78 more matches\n",
        "\n",
        "\n",
        "levdalba/finalprojectgcp ¬∑ scrape_tiktok/main.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "            raise ValueError(\"SCRAPINGBEE_API_KEY environment variable not set\")\n",
        "        scrapingbee_url = \"https://app.scrapingbee.com/api/v1/\"\n",
        "        params = {\n",
        "            \"api_key\": scrapingbee_api_key,\n",
        "            \"url\": profile_url,\n",
        "            \"render_js\": \"true\",\n",
        "            \"wait\": \"5000\",  # Increased to 5 seconds to ensure thumbnails load\n",
        "Show 40 more matches\n",
        "\n",
        "\n",
        "Sowmya-473/BAEONN ¬∑ amazon.py\n",
        "Python\n",
        "¬∑\n",
        "main\n",
        "# ======= CONFIG =======\n",
        "SCRAPINGBEE_API_KEY = 'PO7HX1AE9RP0HJB2RY1BC65FDFP62PO14EZ0OJQU606YYXDEI1UZWC3NWP3PM0QYV8838KGEQQX989XM'  # üîÅ Replace w‚Ä¶\n",
        "HEADERS = {'User-Agent': 'Mozilla/5.0'}\n",
        "BASE_URL = \"https://app.scrapingbee.com/api/v1/\"\n",
        "# ======= USER INPUT =======\n",
        "query = input(\"Enter the product you want to search on Amazon.in: \").strip()\n",
        "encoded_query = urllib.parse.quote_plus(query)\n",
        "search_url = f'https://www.amazon.in/s?k={encoded_query}'\n",
        "Show 52 more matches\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "pattern = r'\\b[A-Z0-9]{20,100}\\b'\n",
        "found_keys = set(re.findall(pattern, myText))\n",
        "print(f\"Total keys found: {len(found_keys)}\")\n",
        "\n",
        "valid_keys = []\n",
        "for key in found_keys:\n",
        "    is_valid = validate_api_key(key)\n",
        "    if is_valid:\n",
        "        valid_keys.append(key)\n",
        "\n",
        "for valid in valid_keys:\n",
        "    print(valid)\n"
      ]
    }
  ]
}