{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6A0mgm/fX+Fhve0JrQlbI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/amien-scrapers/blob/main/Api_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOHcLEP3WmfG",
        "outputId": "271d1dfc-540d-42a3-bcaa-cfa1357fea58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Email Finder Example ===\n",
            "üë§ Finding email for Sam Altman at openai.com\n",
            "{'errors': [{'id': 'too_many_requests', 'code': 429, 'details': \"You've reached the limit for the number of searches per billing period included in your plan.\"}]}\n"
          ]
        }
      ],
      "source": [
        "#@title Hunter.io\n",
        "# 45befd45b616793cbde9cf2142bdc401be79e168\n",
        "# 89afbf8e860cc8a9af6d8c66d0bfd0844815af16\n",
        "# 42cabbf6c8cc2a371ee42414aa7783e500675f62\n",
        "# 0d4701b3e730cbd1b9cd421d358ccbab97f409ca\n",
        "# 5daa2ca116eebcb451f0736dc888126983c3f97d\n",
        "\n",
        "import requests\n",
        "\n",
        "API_KEY = \"09af227de56528b9ca4b336499282d51a32a3477\"\n",
        "BASE_URL = \"https://api.hunter.io/v2\"\n",
        "\n",
        "def domain_search(domain):\n",
        "    \"\"\"Find email addresses related to a domain.\"\"\"\n",
        "    url = f\"{BASE_URL}/domain-search\"\n",
        "    params = {\n",
        "        \"domain\": domain,\n",
        "        \"api_key\": API_KEY\n",
        "    }\n",
        "    print(f\"üîç Searching domain: {domain}\")\n",
        "    response = requests.get(url, params=params)\n",
        "    return response.json()\n",
        "\n",
        "def email_finder(name, domain):\n",
        "    \"\"\"Find the email address of a specific person at a domain.\"\"\"\n",
        "    url = f\"{BASE_URL}/email-finder\"\n",
        "    params = {\n",
        "        \"full_name\": name,\n",
        "        \"domain\": domain,\n",
        "        \"api_key\": API_KEY\n",
        "    }\n",
        "    print(f\"üë§ Finding email for {name} at {domain}\")\n",
        "    response = requests.get(url, params=params)\n",
        "    return response.json()\n",
        "\n",
        "def email_verifier(email):\n",
        "    \"\"\"Verify if an email address is deliverable and valid.\"\"\"\n",
        "    url = f\"{BASE_URL}/email-verifier\"\n",
        "    params = {\n",
        "        \"email\": email,\n",
        "        \"api_key\": API_KEY\n",
        "    }\n",
        "    print(f\"‚úÖ Verifying email: {email}\")\n",
        "    response = requests.get(url, params=params)\n",
        "    return response.json()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example Usage\n",
        "    # print(\"\\n=== Domain Search Example ===\")\n",
        "    # domain_result = domain_search(\"openai.com\")\n",
        "    # print(domain_result)\n",
        "\n",
        "    print(\"\\n=== Email Finder Example ===\")\n",
        "    finder_result = email_finder(\"Sam Altman\", \"openai.com\")\n",
        "    print(finder_result)\n",
        "\n",
        "    # print(\"\\n=== Email Verifier Example ===\")\n",
        "    # verifier_result = email_verifier(\"contact@openai.com\")\n",
        "    # print(verifier_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "class ZenScrapeAPI:\n",
        "    \"\"\"\n",
        "    Simple Python wrapper for the Zenscrape API.\n",
        "    Docs: https://zenscrape.com/docs/\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://app.zenscrape.com/api/v1/get\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def scrape(self, url: str, **kwargs):\n",
        "        \"\"\"\n",
        "        Scrape a web page using Zenscrape API.\n",
        "        Example optional params:\n",
        "          render: true/false (for JS rendering)\n",
        "          premium: true/false\n",
        "          device: 'desktop' or 'mobile'\n",
        "          location: 'us', 'de', etc.\n",
        "        \"\"\"\n",
        "        headers = {\n",
        "            \"apikey\": self.api_key\n",
        "        }\n",
        "\n",
        "        params = {\"url\": url}\n",
        "        params.update(kwargs)\n",
        "\n",
        "        try:\n",
        "            print(f\"[+] Scraping {url} via Zenscrape...\")\n",
        "            response = requests.get(self.BASE_URL, headers=headers, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"[!] Zenscrape request failed: {e}\")\n",
        "            return None\n",
        "\n",
        "        print(\"[‚úÖ] Success ‚Äî data received.\")\n",
        "        return response.text\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # üîß Replace with your actual Zenscrape API key\n",
        "    API_KEY = \"a4e5d860-81a7-11ec-9ded-e71956a8a3831\"\n",
        "\n",
        "    scraper = ZenScrapeAPI(API_KEY)\n",
        "\n",
        "    html = scraper.scrape(\n",
        "        \"https://example.com\",\n",
        "        render=\"false\",      # set to 'true' to enable JavaScript rendering\n",
        "        location=\"us\",       # choose proxy location\n",
        "        premium=\"false\"      # use premium proxies if needed\n",
        "    )\n",
        "\n",
        "    if html:\n",
        "        print(\"First 300 characters of scraped page:\")\n",
        "        print(html[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tWyR2Zxqngo",
        "outputId": "f7697c8f-e871-4b61-f3eb-04a7eb53e73b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Scraping https://example.com via Zenscrape...\n",
            "[!] Zenscrape request failed: 403 Client Error: Forbidden for url: https://app.zenscrape.com/api/v1/get?url=https%3A%2F%2Fexample.com&render=false&location=us&premium=false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# //\"https://api.scraperapi.com?api_key=bb44097292137568bf5b686c1e8d73dc&url=\".concat(url)\n",
        "# //\"https://api.webscrapingapi.com/v1?api_key=bzJh1WiHYl14pC1ndXHFjIMqO8MTaZmG&url=\".concat(url)\n",
        "# //'https://api.allorigins.win/get?url=' + encodeURIComponent(url) + getJSON\n",
        "# // \"https://app.zenscrape.com/api/v1/get?apikey=5868e050-d37c-11eb-8473-6153a26b8aed&url=\".concat(url)\n",
        "# // \"https://api.scrapingdog.com/scrape?api_key=60d64e6d203da4515bf8e50f&url=\".concat(url)\n"
      ],
      "metadata": {
        "id": "dYt6n6zTwK3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FI5MwJFYW1Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7f104023de9b3d431f861fd2d6fe0a499f717636fb3a65f3ab6fd70e7ed2a966\n",
        "\n",
        "import requests\n",
        "\n",
        "class SerpAPI:\n",
        "    \"\"\"\n",
        "    Simple Python wrapper for the SerpAPI service.\n",
        "    Docs: https://serpapi.com/search-api\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://serpapi.com/search\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def search(self, query: str, engine: str = \"google\", **kwargs):\n",
        "        \"\"\"\n",
        "        Perform a search query via SerpAPI.\n",
        "\n",
        "        Args:\n",
        "            query (str): The search term.\n",
        "            engine (str): Search engine (default: google).\n",
        "                Supported engines: google, bing, youtube, google_maps, etc.\n",
        "            **kwargs: Additional SerpAPI parameters like:\n",
        "                location, hl, gl, num, start, etc.\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            \"api_key\": self.api_key,\n",
        "            \"engine\": engine,\n",
        "            \"q\": query,\n",
        "        }\n",
        "        params.update(kwargs)\n",
        "\n",
        "        try:\n",
        "            print(f\"[+] Searching '{query}' on {engine} via SerpAPI...\")\n",
        "            response = requests.get(self.BASE_URL, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"[!] SerpAPI request failed: {e}\")\n",
        "            return None\n",
        "\n",
        "        print(\"[‚úÖ] Search successful.\")\n",
        "        return response.json()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # üîß Replace with your actual SerpAPI key\n",
        "    API_KEY = \"7f104023de9b3d431f861fd2d6fe0a499f717636fb3a65f3ab6fd70e7ed2a966\"\n",
        "\n",
        "    serp = SerpAPI(API_KEY)\n",
        "\n",
        "    # Example 1: Google search\n",
        "    results = serp.search(\"OpenAI ChatGPT\", engine=\"google\", num=5)\n",
        "\n",
        "    if results:\n",
        "        # Print top results\n",
        "        organic_results = results.get(\"organic_results\", [])\n",
        "        print(f\"\\nFound {len(organic_results)} results.\\n\")\n",
        "        for i, r in enumerate(organic_results[:5], 1):\n",
        "            print(f\"{i}. {r.get('title')}\")\n",
        "            print(f\"   {r.get('link')}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLc9fFiFX4_U",
        "outputId": "458094d6-ac8b-4d0e-d643-3ad8c7090ff2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Searching 'OpenAI ChatGPT' on google via SerpAPI...\n",
            "[‚úÖ] Search successful.\n",
            "\n",
            "Found 4 results.\n",
            "\n",
            "1. OpenAI\n",
            "   https://openai.com/\n",
            "\n",
            "2. ChatGPT\n",
            "   https://chatgpt.com/\n",
            "\n",
            "3. ChatGPT on the App Store\n",
            "   https://apps.apple.com/us/app/chatgpt/id6448311069\n",
            "\n",
            "4. AI Chatbot to Discover, Learn & Create\n",
            "   https://chatgpt.com/overview/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "class SerperAPI:\n",
        "    \"\"\"\n",
        "    Simple Python wrapper for the Serper.dev API.\n",
        "    Docs: https://serper.dev\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://google.serper.dev/search\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def search(self, query: str, **kwargs):\n",
        "        \"\"\"\n",
        "        Perform a Google search using Serper.dev API.\n",
        "\n",
        "        Args:\n",
        "            query (str): The search term.\n",
        "            **kwargs: Optional parameters such as:\n",
        "                - gl: country code (e.g., 'us', 'id')\n",
        "                - hl: language (e.g., 'en', 'id')\n",
        "                - num: number of results\n",
        "                - type: 'news', 'images', etc.\n",
        "        \"\"\"\n",
        "        headers = {\n",
        "            \"X-API-KEY\": self.api_key,\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        payload = {\"q\": query}\n",
        "        payload.update(kwargs)\n",
        "\n",
        "        try:\n",
        "            print(f\"[+] Searching '{query}' via Serper.dev...\")\n",
        "            response = requests.post(self.BASE_URL, headers=headers, json=payload, timeout=30)\n",
        "            response.raise_for_status()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"[!] Serper API request failed: {e}\")\n",
        "            return None\n",
        "\n",
        "        print(\"[‚úÖ] Search successful.\")\n",
        "        return response.json()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # üîß Replace with your actual Serper API key\n",
        "    API_KEY = \"dd6e2cade970164395726665088c36201b0c1c89\"\n",
        "\n",
        "    serper = SerperAPI(API_KEY)\n",
        "\n",
        "    # Example: basic Google search\n",
        "    results = serper.search(\"OpenAI ChatGPT\", gl=\"us\", hl=\"en\")\n",
        "\n",
        "    if results and \"organic\" in results:\n",
        "        organic = results[\"organic\"]\n",
        "        print(f\"\\nFound {len(organic)} organic results:\\n\")\n",
        "        for i, r in enumerate(organic[:5], 1):\n",
        "            print(f\"{i}. {r.get('title')}\")\n",
        "            print(f\"   {r.get('link')}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0oToMQ2YlNk",
        "outputId": "caacba89-7e13-4ab5-d746-5180d2626119"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Searching 'OpenAI ChatGPT' via Serper.dev...\n",
            "[!] Serper API request failed: 403 Client Error: Forbidden for url: https://google.serper.dev/search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class ScrapingDogAPI:\n",
        "    \"\"\"\n",
        "    Simple Python wrapper for ScrapingDog API.\n",
        "    Docs: https://www.scrapingdog.com/documentation\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.scrapingdog.com/scrape\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def scrape(self, url: str, **kwargs):\n",
        "        \"\"\"\n",
        "        Fetch webpage content via ScrapingDog.\n",
        "        Example optional params:\n",
        "          - render_js: true/false\n",
        "          - premium: true/false\n",
        "          - proxy_country: 'us', 'de', etc.\n",
        "        \"\"\"\n",
        "        params = {\"api_key\": self.api_key, \"url\": url}\n",
        "        params.update(kwargs)\n",
        "\n",
        "        try:\n",
        "            print(f\"[+] Scraping: {url} via ScrapingDog...\")\n",
        "            response = requests.get(self.BASE_URL, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"[!] Request failed: {e}\")\n",
        "            return None\n",
        "\n",
        "        print(\"[‚úÖ] Success ‚Äî HTML data received.\")\n",
        "        return response.text\n",
        "\n",
        "\n",
        "def parse_books(html: str):\n",
        "    \"\"\"\n",
        "    Example parser: extract book titles and prices from 'books.toscrape.com'\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    books = []\n",
        "\n",
        "    for article in soup.select(\"article.product_pod\"):\n",
        "        title = article.h3.a[\"title\"]\n",
        "        price = article.select_one(\".price_color\").text\n",
        "        books.append({\"title\": title, \"price\": price})\n",
        "\n",
        "    return books\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # üîß Replace with your actual ScrapingDog API key\n",
        "    API_KEY = \"68dd8e164d5c229c193df9d2\"\n",
        "\n",
        "    scraper = ScrapingDogAPI(API_KEY)\n",
        "    target_url = \"https://example.com\"\n",
        "\n",
        "    html = scraper.scrape(target_url, render_js=\"false\")\n",
        "\n",
        "    if html:\n",
        "        print(\"First 300 characters of scraped page:\")\n",
        "        print(html[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vBPruv1eg2c",
        "outputId": "cd0b86da-c586-46cd-c3b6-390592d372b7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+] Scraping: https://example.com via ScrapingDog...\n",
            "[!] Request failed: 400 Client Error: Bad Request for url: https://api.scrapingdog.com/scrape?api_key=68dd8e164d5c229c193df9d2&url=https%3A%2F%2Fexample.com&render_js=false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get(\"https://api.scrapingdog.com/scrape?api_key=5ea541dcacf6581b0b4b4042&url=https://www.mims.com/india/drug\")\n",
        "r.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmAdji05iTdl",
        "outputId": "cad3fc2c-141c-44c2-e268-0f5285e56ad8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'{\"message\":\"Limit reached. For help email us at info@scrapingdog.com\",\"success\":false}'"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}