{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMguvQKzHrGJIN+EBSv3kag"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pSwzdY79Wse",
        "outputId": "efaf0ba5-6022-4af2-f598-3033be4c5bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping https://books.toscrape.com/ through ScrapeOps proxy...\n",
            "âŒ Error: 401 {\"Error\":\"The API key you sent with the request is invalid. Please include a valid API key.\"}\n",
            "âœ… Found 0 book titles:\n",
            "\n",
            "ðŸ’¾ Saved HTML to books_scraped.html\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "SCRAPEOPS_API_KEY = \"\"\n",
        "TARGET_URL = \"https://books.toscrape.com/\"\n",
        "SCRAPEOPS_ENDPOINT = \"https://proxy.scrapeops.io/v1/\"\n",
        "\n",
        "# === BUILD REQUEST PARAMETERS ===\n",
        "params = {\n",
        "    \"api_key\": SCRAPEOPS_API_KEY,\n",
        "    \"url\": TARGET_URL\n",
        "}\n",
        "\n",
        "print(f\"Scraping {TARGET_URL} through ScrapeOps proxy...\")\n",
        "\n",
        "# === SEND REQUEST ===\n",
        "response = requests.get(SCRAPEOPS_ENDPOINT, params=params)\n",
        "\n",
        "if response.status_code != 200:\n",
        "    print(\"âŒ Error:\", response.status_code, response.text)\n",
        "    exit()\n",
        "\n",
        "# === PARSE HTML USING BEAUTIFULSOUP ===\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Example extraction: titles of books\n",
        "titles = [h3.get_text(strip=True) for h3 in soup.select(\"h3 > a\")]\n",
        "\n",
        "print(f\"âœ… Found {len(titles)} book titles:\")\n",
        "for i, title in enumerate(titles[:10], start=1):\n",
        "    print(f\"{i}. {title}\")\n",
        "\n",
        "# === OPTIONAL: SAVE HTML TO FILE ===\n",
        "with open(\"books_scraped.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "    print(\"\\nðŸ’¾ Saved HTML to books_scraped.html\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_api_key(api_key):\n",
        "  TARGET_URL = \"https://books.toscrape.com/\"\n",
        "  SCRAPEOPS_ENDPOINT = \"https://proxy.scrapeops.io/v1/\"\n",
        "  params = {\n",
        "      \"api_key\": api_key,\n",
        "      \"url\": TARGET_URL\n",
        "  }\n",
        "  response = requests.get(SCRAPEOPS_ENDPOINT, params=params)\n",
        "\n",
        "  if response.status_code != 200:\n",
        "      return False\n",
        "  else:\n",
        "      return True"
      ],
      "metadata": {
        "id": "jglcejEF970m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "def validate_api_key(api_key):\n",
        "  TARGET_URL = \"https://books.toscrape.com/\"\n",
        "  SCRAPEOPS_ENDPOINT = \"https://proxy.scrapeops.io/v1/\"\n",
        "  params = {\n",
        "      \"api_key\": api_key,\n",
        "      \"url\": TARGET_URL\n",
        "  }\n",
        "  response = requests.get(SCRAPEOPS_ENDPOINT, params=params)\n",
        "\n",
        "  if response.status_code != 200:\n",
        "      return False\n",
        "  else:\n",
        "      return True\n",
        "\n",
        "myText = \"\"\"\n",
        "Skip to content\n",
        "Navigation Menu\n",
        "Search GitHub:\n",
        "http://scrapeops:\n",
        "code Search Results Â· http://scrapeops:\n",
        "Filter by\n",
        "Languages\n",
        "Repositories\n",
        "Paths\n",
        "Advanced\n",
        "20 files\n",
        " (257 ms)\n",
        "20 files\n",
        "\n",
        "\n",
        "gnh1201/welsonjs Â· data/available_proxies.json\n",
        "JSON\n",
        "Â·\n",
        "master\n",
        "    {\n",
        "        \"type\": \"stateful\",\n",
        "        \"provider\": \"scrapeops\",\n",
        "        \"url\": \"http://scrapeops:{api_key}@residential-proxy.scrapeops.io:8181\",\n",
        "        \"documentation\": \"https://scrapeops.io?fpr=namhyeon75\"\n",
        "    },\n",
        "    {\n",
        "\n",
        "\n",
        "githubfarid1/resy-project Â· settings.py.example\n",
        "Â·\n",
        "main\n",
        "MAX_IDLE_TIME = 2\n",
        "PROXY_API_KEY = '6456b40c-0bb9-4277-a24d-82901d1eae56'\n",
        "PROXIES = {\n",
        "    'http': f'http://scrapeops:{PROXY_API_KEY}@residential-proxy.scrapeops.io:8181',\n",
        "    'https': f'http://scrapeops:{PROXY_API_KEY}@residential-proxy.scrapeops.io:8181',\n",
        "}\n",
        "\n",
        "\n",
        "Muyoouu/instagram-scraper Â· main.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "        # Set proxy\n",
        "        cl = Client()\n",
        "        load_dotenv()\n",
        "        cl.set_proxy(f\"http://scrapeops:{environ.get('SCRAPEOPS_API_KEY')}@proxy.scrapeops.io:5353\")\n",
        "        proxy_usage_counter = 0\n",
        "        # Loads target user info data\n",
        "\n",
        "\n",
        "ryanmichaelhirst/optcg-api Â· scripts/scrape-images.ts\n",
        "TypeScript\n",
        "Â·\n",
        "main\n",
        "      // Residential Proxy Aggregator\n",
        "      const proxyAgent = new HttpsProxyAgent(\n",
        "        `http://scrapeops:${password}@residential-proxy.scrapeops.io:8181`,\n",
        "        {\n",
        "          timeout: 120000,\n",
        "        },\n",
        "\n",
        "\n",
        "saagaar/For-thesis-Final---Scraping-car-data Â· hamrobazar-scrape/hamrobazarDetail.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "# proxy_options = {\n",
        "#     'proxy': {\n",
        "#         'http': f'http://scrapeops:{SCRAPEOPS_API_KEY}@proxy.scrapeops.io:5353',\n",
        "#         'https': f'http://scrapeops:{SCRAPEOPS_API_KEY}@proxy.scrapeops.io:5353',\n",
        "#         'no_proxy': 'localhost:127.0.0.1'\n",
        "#     }\n",
        "# }\n",
        "\n",
        "\n",
        "ShaheerAfzal/WebScraping-Project Â· YahooScrape/yScrape/yScrape/spiders/yScraper.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "                \"proxy\": \"http://scrapeops:12c6c1f6-3283-4cf3-a7ab-344f652ae005@residential-proxy.scrapeops.io:8181\"})\n",
        "        #         \"proxy\": \"http://scrapeops:12c6c1f6-3283-4cf3-a7ab-344f652ae005@residential-proxy.scrapeops.io:8181\"})\n",
        "\n",
        "\n",
        "Aldiwildan77/ecommerce-scraping Â· shopee.ipynb\n",
        "Jupyter Notebook\n",
        "Â·\n",
        "master\n",
        "    \"\\n\",\n",
        "    \"def get_proxies_scrapeops():\\n\",\n",
        "    \"    proxies = [\\n\",\n",
        "    \"        \\\"http://scrapeops:13dae7aa-0fa6-4083-816e-f4cb423b9a1a@residential-proxy.scrapeops.io:8181\\\",\\n\",\n",
        "    \"        \\\"http://scrapeops:13dae7aa-0fa6-4083-816e-f4cb423b9a1a@residential-proxy.scrapeops.io:8181\\\"\\n\",\n",
        "    \"    ]\\n\",\n",
        "    \"\\n\",\n",
        "      \"Using proxy: http://scrapeops:13dae7aa-0fa6-4083-816e-f4cb423b9a1a@residential-proxy.scrapeops.io:8181\\n\",\n",
        "\n",
        "\n",
        "githubfarid1/hermesbot-project Â· tester/tester3.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "    # breakpoint()\n",
        "    proxies = {\n",
        "    \"http\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "    \"https\":\"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "}\n",
        "    response = session.get('https://www.hermes.com/us/en/product/lindy-ii-mini-bag-H085956CK55/', headers={\n",
        "\n",
        "\n",
        "githubfarid1/resybot-website Â· testproxy.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "from playwright_stealth import stealth_sync\n",
        "import sys\n",
        "# proxies = {\n",
        "#   \"http\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "#   \"https\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\"\n",
        "# }\n",
        "# response = requests.get('https://quotes.toscrape.com/', proxies=proxies, verify=False)\n",
        "  \"http\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "  \"https\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\"\n",
        "\n",
        "\n",
        "githubfarid1/rtoproject Â· tesproxy.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "}\n",
        "proxies = {\n",
        "  \"http\": \"http://scrapeops:c58f3885-5149-4c03-8a3a-1b2d3ffba3c6@residential-proxy.scrapeops.io:8181\",\n",
        "  \"https\": \"http://scrapeops:c58f3885-5149-4c03-8a3a-1b2d3ffba3c6@residential-proxy.scrapeops.io:8181\"\n",
        "}\n",
        "response = requests.get(\n",
        "    'https://training.gov.au/Search/AjaxOrganisationSearchFiltersCount',\n",
        "\n",
        "\n",
        "baramofme/welsonjs Â· data/available_proxies.json\n",
        "JSON\n",
        "Â·\n",
        "master\n",
        "    {\n",
        "        \"type\": \"stateful\",\n",
        "        \"provider\": \"scrapeops\",\n",
        "        \"url\": \"http://scrapeops:{api_key}@residential-proxy.scrapeops.io:8181\",\n",
        "        \"documentation\": \"https://scrapeops.io?fpr=namhyeon75\"\n",
        "    },\n",
        "    {\n",
        "\n",
        "\n",
        "saagaar/For-thesis-Final---Scraping-car-data Â· For Research paper/3.hamrobazarDetail.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "# proxy_options = {\n",
        "#     'proxy': {\n",
        "#         'http': f'http://scrapeops:{SCRAPEOPS_API_KEY}@proxy.scrapeops.io:5353',\n",
        "#         'https': f'http://scrapeops:{SCRAPEOPS_API_KEY}@proxy.scrapeops.io:5353',\n",
        "#         'no_proxy': 'localhost:127.0.0.1'\n",
        "#     }\n",
        "# }\n",
        "\n",
        "\n",
        "githubfarid1/hermesbot-project Â· bypass-datadome/datadome_bin.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "        \"http\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "        \"https\":\"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "\n",
        "\n",
        "githubfarid1/resy-project Â· README.TXT\n",
        "Text\n",
        "Â·\n",
        "main\n",
        "format residential proxy:\n",
        "PROXY_TYPE://USERNAME:PASSWORD@PROXY_ADDRESS:PORT_NUMBER\n",
        "example:\n",
        "PROXIES = {\n",
        "    'http': f'http://scrapeops:6456b40c-0bb9-4277-a24d-82901d1eae56@residential-proxy.scrapeops.io:8181',\n",
        "    'https': f'http://scrapeops:6456b40c-0bb9-4277-a24d-82901d1eae56@residential-proxy.scrapeops.io:8181',\n",
        "}\n",
        "\n",
        "\n",
        "githubfarid1/resybot-website Â· botmodules/update_token.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "}\n",
        "PROXY_REQUEST = {\n",
        "    \"http\":\"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "    \"https\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\"\n",
        "}\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "githubfarid1/resy-project Â· proxylist.json.txt\n",
        "Text\n",
        "Â·\n",
        "main\n",
        "[{\"profilename\": \"scrapeops.io\", \"http_proxy\": \"http://scrapeops:c58f3885-5149-4c03-8a3a-1b2d3ffba3c6@residential-proxyâ€¦\n",
        "\n",
        "\n",
        "githubfarid1/resybot-website Â· botmodules/resybotcheckbooking_old.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "}\n",
        "PROXY_REQUEST = {\n",
        "    \"http\":\"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "    \"https\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\"\n",
        "}\n",
        "# \"rgs://resy/64869/1802535/2/2024-09-13/2024-09-13/21:30:00/2/Dining+room\"\n",
        "\n",
        "\n",
        "githubfarid1/hermesbot-project Â· datadome-gen/main.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "}\n",
        "proxies = {\n",
        "    \"http\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "    \"https\":\"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "}\n",
        "headers['datadome'] = cookiefix\n",
        "while True:\n",
        "\n",
        "\n",
        "githubfarid1/resy-project Â· tesproxy.py\n",
        "Python\n",
        "Â·\n",
        "main\n",
        "# proxies = {\n",
        "#   \"http\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\",\n",
        "#   \"https\": \"http://scrapeops:f2d43fe5-5bee-41ab-83f9-da70ae59c60a@residential-proxy.scrapeops.io:8181\"\n",
        "# }\n",
        "# response = requests.get('https://quotes.toscrape.com/', proxies=proxies, verify=False)\n",
        "# print(response.text)\n",
        "\n",
        "\n",
        "saagaar/For-thesis-Final---Scraping-car-data Â· hamrobazar-scrape/test.ipynb\n",
        "Jupyter Notebook\n",
        "Â·\n",
        "main\n",
        "# proxy_options = {\n",
        "#     'proxy': {\n",
        "#         'http': f'http://scrapeops:{SCRAPEOPS_API_KEY}@proxy.scrapeops.io:5353',\n",
        "#         'https': f'http://scrapeops:{SCRAPEOPS_API_KEY}@proxy.scrapeops.io:5353',\n",
        "#         'no_proxy': 'localhost:127.0.0.1'\n",
        "#     }\n",
        "# }\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "pattern = r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}'\n",
        "\n",
        "api_keys = re.findall(pattern, myText)\n",
        "api_keys = set(list(api_keys))\n",
        "print(len(api_keys))\n",
        "\n",
        "for key in api_keys:\n",
        "    is_valid = validate_api_key(key)\n",
        "    if is_valid:\n",
        "        print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHqgsKfh-Rnf",
        "outputId": "1b247af5-177a-4f8b-a861-a63a209c988e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "er7oiQoJ_6bv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}