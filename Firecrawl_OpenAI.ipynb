{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbjEksvCfiPMD0NfKIADGL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/amien-scrapers/blob/main/Firecrawl_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4piwDSSZ5o9W",
        "outputId": "548af111-a0e9-417f-f68c-956e0ff16ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting firecrawl-py\n",
            "  Downloading firecrawl_py-1.2.4-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.46.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (2.32.3)\n",
            "Collecting python-dotenv (from firecrawl-py)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting websockets (from firecrawl-py)\n",
            "  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (1.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2.0.7)\n",
            "Downloading firecrawl_py-1.2.4-py3-none-any.whl (15 kB)\n",
            "Downloading openai-1.46.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.0/375.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, python-dotenv, jiter, h11, httpcore, firecrawl-py, httpx, openai\n",
            "Successfully installed firecrawl-py-1.2.4 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.46.0 python-dotenv-1.0.1 websockets-13.0.1\n"
          ]
        }
      ],
      "source": [
        "# Install necessary modules\n",
        "!pip install firecrawl-py openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "from firecrawl import FirecrawlApp\n",
        "\n",
        "# ANSI color codes\n",
        "class Colors:\n",
        "    CYAN = '\\033[96m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    GREEN = '\\033[92m'\n",
        "    RED = '\\033[91m'\n",
        "    MAGENTA = '\\033[95m'\n",
        "    BLUE = '\\033[94m'\n",
        "    RESET = '\\033[0m'\n",
        "\n",
        "# Retrieve API keys from environment variables\n",
        "firecrawl_api_key = \"fc-837e8edb2b004354a94de92938d733b9\"\n",
        "openai_api_key = \"\"\n",
        "\n",
        "# Initialize the FirecrawlApp and OpenAI client\n",
        "app = FirecrawlApp(api_key=firecrawl_api_key)\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Find the page that most likely contains the objective\n",
        "def find_relevant_page_via_map(objective, url, app, client):\n",
        "    try:\n",
        "        print(f\"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}\")\n",
        "        print(f\"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}\")\n",
        "\n",
        "        map_prompt = f\"\"\"\n",
        "        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}\")\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": map_prompt\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        map_search_parameter = completion.choices[0].message.content\n",
        "        print(f\"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}\")\n",
        "\n",
        "        print(f\"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}\")\n",
        "        map_website = app.map_url(url, params={\"search\": map_search_parameter})\n",
        "        print(f\"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}\")\n",
        "        print(f\"{Colors.GREEN}Located {len(map_website)} relevant links.{Colors.RESET}\")\n",
        "        return map_website\n",
        "    except Exception as e:\n",
        "        print(f\"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}\")\n",
        "        return None\n",
        "\n",
        "# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None\n",
        "def find_objective_in_top_pages(map_website, objective, app, client):\n",
        "    try:\n",
        "        # Get top 3 links from the map result\n",
        "        top_links = map_website[:3] if isinstance(map_website, list) else []\n",
        "        print(f\"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}\")\n",
        "\n",
        "        for link in top_links:\n",
        "            print(f\"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}\")\n",
        "            # Scrape the page\n",
        "            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})\n",
        "            print(f\"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}\")\n",
        "\n",
        "\n",
        "            # Check if objective is met\n",
        "            check_prompt = f\"\"\"\n",
        "            Given the following scraped content and objective, determine if the objective is met.\n",
        "            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.\n",
        "            If the objective is not met with confidence, respond with 'Objective not met'.\n",
        "\n",
        "            Objective: {objective}\n",
        "            Scraped content: {scrape_result['markdown']}\n",
        "\n",
        "            Remember:\n",
        "            1. Only return JSON if you are confident the objective is fully met.\n",
        "            2. Keep the JSON structure as simple and flat as possible.\n",
        "            3. Do not include any explanations or markdown formatting in your response.\n",
        "            \"\"\"\n",
        "\n",
        "            completion = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": check_prompt\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            result = completion.choices[0].message.content\n",
        "\n",
        "            if result != \"Objective not met\":\n",
        "                print(f\"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}\")\n",
        "                try:\n",
        "                    return json.loads(result)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}\")\n",
        "            else:\n",
        "                print(f\"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}\")\n",
        "\n",
        "        print(f\"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}\")\n",
        "        return None\n",
        "\n",
        "# Main function to execute the process\n",
        "def main():\n",
        "    # Get user input\n",
        "    url = input(f\"{Colors.BLUE}Enter the website to crawl: {Colors.RESET}\")\n",
        "    objective = input(f\"{Colors.BLUE}Enter your objective: {Colors.RESET}\")\n",
        "\n",
        "    print(f\"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}\")\n",
        "    # Find the relevant page\n",
        "    map_website = find_relevant_page_via_map(objective, url, app, client)\n",
        "\n",
        "    if map_website:\n",
        "        print(f\"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis...{Colors.RESET}\")\n",
        "        # Find objective in top pages\n",
        "        result = find_objective_in_top_pages(map_website, objective, app, client)\n",
        "\n",
        "        if result:\n",
        "            print(f\"{Colors.GREEN}Objective successfully fulfilled. Extracted information:{Colors.RESET}\")\n",
        "            print(f\"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}\")\n",
        "        else:\n",
        "            print(f\"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}\")\n",
        "    else:\n",
        "        print(f\"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}\")\n"
      ],
      "metadata": {
        "id": "1TxQ374D5wNs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abclOaTs6qN7",
        "outputId": "bc2b6b9d-2cc3-468c-e244-033713b24fac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mEnter the website to crawl: \u001b[0mhttps://solonamerica.com/\n",
            "\u001b[94mEnter your objective: \u001b[0m find out if the company has offerings in multiple states,  record the states, the offers, the past project, current project, future project (project name & city/state)\n",
            "\u001b[93mInitiating web crawling process...\u001b[0m\n",
            "\u001b[96mUnderstood. The objective is:  find out if the company has offerings in multiple states,  record the states, the offers, the past project, current project, future project (project name & city/state)\u001b[0m\n",
            "\u001b[96mInitiating search on the website: https://solonamerica.com/\u001b[0m\n",
            "\u001b[93mAnalyzing objective to determine optimal search parameter...\u001b[0m\n",
            "\u001b[91mError encountered during relevant page identification: Error code: 400 - {'error': {'message': \"Your organization must qualify for at least usage tier 5 to access 'o1-preview'. See https://platform.openai.com/docs/guides/rate-limits/usage-tiers for more details on usage tiers.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'below_usage_tier'}}\u001b[0m\n",
            "\u001b[91mNo relevant pages identified. Consider refining the search parameters or trying a different website.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC3fFMBJ8zR2",
        "outputId": "c370f302-9c29-490b-e999-e392ab9e0b34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mEnter the website to crawl: \u001b[0mhttps://solonamerica.com/\n",
            "\u001b[94mEnter your objective: \u001b[0mfind out if the company has offerings in multiple states,  record the states, the offers, the past project, current project, future project (project name & city/state)\n",
            "\u001b[93mInitiating web crawling process...\u001b[0m\n",
            "\u001b[96mUnderstood. The objective is: find out if the company has offerings in multiple states,  record the states, the offers, the past project, current project, future project (project name & city/state)\u001b[0m\n",
            "\u001b[96mInitiating search on the website: https://solonamerica.com/\u001b[0m\n",
            "\u001b[93mAnalyzing objective to determine optimal search parameter...\u001b[0m\n",
            "\u001b[92mOptimal search parameter identified: state projects\u001b[0m\n",
            "\u001b[93mMapping website using the identified search parameter...\u001b[0m\n",
            "\u001b[92mWebsite mapping completed successfully.\u001b[0m\n",
            "\u001b[92mLocated 1 relevant links.\u001b[0m\n",
            "\u001b[92mRelevant pages identified. Proceeding with detailed analysis...\u001b[0m\n",
            "\u001b[96mProceeding to analyze top 1 links: ['https://solonamerica.com']\u001b[0m\n",
            "\u001b[93mInitiating scrape of page: https://solonamerica.com\u001b[0m\n",
            "\u001b[92mPage scraping completed successfully.\u001b[0m\n",
            "\u001b[92mObjective potentially fulfilled. Relevant information identified.\u001b[0m\n",
            "\u001b[92mObjective successfully fulfilled. Extracted information:\u001b[0m\n",
            "\u001b[95m{\n",
            "  \"Company\": \"SOLON\",\n",
            "  \"States\": [\n",
            "    \"Arizona\"\n",
            "  ],\n",
            "  \"Offers\": {\n",
            "    \"Solar/Photovoltaics\": {\n",
            "      \"Description\": \"Turnkey solar photovoltaic (PV) system solutions with industry-leading innovations, flexible solutions tailored for each project, and quality systems delivered safely, on-time, and on-budget.\",\n",
            "      \"Link\": \"/solar-photovoltaics/\"\n",
            "    },\n",
            "    \"Energy Storage & More\": {\n",
            "      \"Description\": \"Expertise in integrating complementary technologies like energy storage and microgrid solutions, grid support solutions, and providing turnkey projects including EV Chargers and LED lighting.\",\n",
            "      \"Link\": \"/energy-storage-more/\"\n",
            "    }\n",
            "  },\n",
            "  \"Projects\": {\n",
            "    \"Past\": {\n",
            "      \"Name\": \"City of Mesa ASU Project\",\n",
            "      \"Location\": \"Mesa, Arizona\"\n",
            "    },\n",
            "    \"Current\": {\n",
            "      \"Name\": \"Grand Canyon West Solar + Storage Microgrid\",\n",
            "      \"Location\": \"Grand Canyon West\"\n",
            "    },\n",
            "    \"Future\": {\n",
            "      \"Name\": \"SEAWORLD SAN ANTONIO 4.3 MW Solar Canopy\",\n",
            "      \"Location\": \"San Antonio, Texas\"\n",
            "    }\n",
            "  }\n",
            "}\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}