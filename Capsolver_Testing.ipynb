{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/amien-scrapers/blob/main/Capsolver_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEtSPfkQcGW6",
        "outputId": "13791b75-369b-4771-f915-fd76b27be20f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'errorCode': 'ERROR_PROXY_CONNECT_REFUSED',\n",
              " 'errorDescription': 'custom proxy connect failed.',\n",
              " 'errorId': 1}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "api_key = \"CAP-5DA091C94C88DBE90BFB1D2AFADF8212697C53BC04A80D46DFC930238F0C74FE\"\n",
        "\n",
        "payload = {\n",
        "    \"clientKey\": api_key,\n",
        "    \"task\": {\n",
        "        \"type\": 'DatadomeSliderTask',\n",
        "        \"websiteURL\": \"https://www.hermes.com\",\n",
        "        \"captchaUrl\": \"https://geo.captcha-delivery.com/captcha/?initialCid=AHrlqAAAAAMA1QGvUmJwyYoAwpyjNg%3D%3D&hash=789361B674144528D0B7EE76B35826&cid=6QAEcL8coBYTi9tYLmjCdyKmNNyHz1xwM2tMHHGVd_Rxr6FsWrb7H~a04csMptCPYfQ25CBDmaOZpdDa4qwAigFnsrzbCkVkoaBIXVAwHsjXJaKYXsTpkBPtqJfLMGN&t=fe&referer=https%3A//www.hermes.com/us/en/product/lindy-ii-mini-bag-H085956CK55/\",\n",
        "        \"proxy\": \"http://45.238.58.161:8080\",\n",
        "    \"userAgent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "}\n",
        "res = requests.post(\"https://api.capsolver.com/createTask\", json=payload)\n",
        "resp = res.json()\n",
        "resp\n",
        "# task_id = resp.get(\"taskId\")\n",
        "# if not task_id:\n",
        "#     print(\"Failed to create task:\", res.text)\n",
        "#     return\n",
        "# print(f\"Got taskId: {task_id} / Getting result...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Me1hANzqiXJG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "ff2a565b-0c1b-4eb0-fc47-5a0dbd60b771"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "AuthenticationError: Your api key is invalid or balance is insufficient (CAP-EC5F3697B76BC9FCD54AC4AF7037634B).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-241045981.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcapsolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CAP-EC5F3697B76BC9FCD54AC4AF7037634B'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m solution = capsolver.solve({\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ReCaptchaV2TaskProxyLess\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;34m\"websiteKey\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"6Le-wvkSAAAAAPBMRTvw0Q4Muexq9bi0DJwx_mJ-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/capsolver/capsolver.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(cls, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapsolverError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'solution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: AuthenticationError: Your api key is invalid or balance is insufficient (CAP-EC5F3697B76BC9FCD54AC4AF7037634B)."
          ]
        }
      ],
      "source": [
        "# CAP-5DA091C94C88DBE90BFB1D2AFADF8212697C53BC04A80D46DFC930238F0C74FE\n",
        "# CAP-D2D4BC1B86FD4F550ED83C329898264E02F0E2A7A81E1B079F64F7F11477C8FD\n",
        "# CAP-BD48765631E316FCA364D5F2F776E224\n",
        "# CAP-F1E9FC03922E38944D9626D7A57AAAE0\n",
        "# CAP-\n",
        "\n",
        "\n",
        "# !pip install --upgrade capsolver\n",
        "\n",
        "import capsolver\n",
        "\n",
        "capsolver.api_key = 'CAP-EC5F3697B76BC9FCD54AC4AF7037634B'\n",
        "\n",
        "solution = capsolver.solve({\n",
        "  \"type\": \"ReCaptchaV2TaskProxyLess\",\n",
        "  \"websiteKey\": \"6Le-wvkSAAAAAPBMRTvw0Q4Muexq9bi0DJwx_mJ-\",\n",
        "  \"websiteURL\": \"https://www.google.com/recaptcha/api2/demo\"\n",
        "})\n",
        "\n",
        "print(solution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib59hOyFkTo_"
      },
      "outputs": [],
      "source": [
        "#@title Oxylabs Testing\n",
        "\n",
        "import requests\n",
        "\n",
        "USERNAME = \"customer-ecuachecks-cc-ec-sessid-0620968049-sesstime-3\"\n",
        "PASSWORD = \"Ecuachecks2023\"\n",
        "\n",
        "session_obj = requests.Session()\n",
        "session_obj.proxies = {\n",
        "    \"http\": f\"http://{USERNAME}:{PASSWORD}@pr.oxylabs.io:7777\",\n",
        "    \"https\": f\"http://{USERNAME}:{PASSWORD}@pr.oxylabs.io:7777\"\n",
        "}\n",
        "\n",
        "response = session_obj.get(\"https://ip.oxylabs.io/\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKlGS7jg5pAd"
      },
      "outputs": [],
      "source": [
        "# spaua2jty5:ze1+cs06uiOYw0nWrJ\n",
        "# spoj2y7udg:mKWn2e~qhkHX6uql27\n",
        "# user-sp3j58curv-session-1-country-gb:9oOoKQ8+z8pkcUsnv0\n",
        "# sp3j58curv:9oOoKQ8+z8pkcUsnv0\n",
        "# spimz9t8no:fvFs72xP+Tr4bK1ubj\n",
        "# user-sp3j58curv:9oOoKQ8+z8pkcUsnv0\n",
        "# sp2gah7537:d7bHGuL3f4f3hNp~bp@gate.decodo.com:10001\n",
        "# sprf7np0xf:nqOKJvbU+v008x1nox@gate.decodo.com:10001\n",
        "# user-sp3j58curv-sessionduration-60-country-uk:9oOoKQ8+z8pkcUsnv0@gate.decodo.com:42690\n",
        "# spmhmfozio:ze1sg+sP3n4apXhDV9@isp.decodo.com:10001\n",
        "\n",
        "import requests\n",
        "url = 'https://ip.decodo.com/json'\n",
        "username = 'username'\n",
        "password = 'password'\n",
        "proxy = f\"http://user-sp3j58curv-sessionduration-60-country-gb:9oOoKQ8+z8pkcUsnv0@gate.decodo.com:10001\"\n",
        "result = requests.get(url, proxies = {\n",
        "    'http': proxy,\n",
        "    'https': proxy\n",
        "})\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-fYmoDOwcyq",
        "outputId": "d4446c9a-acd8-466e-a1c4-eb2a69033c9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'message': 'You have reached your account limit. To add more API credits upgrade your account from here https://api.scrapingdog.com/billing. ',\n",
              " 'success': False}"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Scrapingdog\n",
        "# 63655678cd36805b2cb76220\n",
        "# 68cf4281d592f35c3199242b\n",
        "# 68345b67b24e285fb8e70926\n",
        "# 68dd8e164d5c229c193df9d2\n",
        "# 67e77e309f7ca77634a930e7\n",
        "\n",
        "import requests\n",
        "\n",
        "api_key = \"65fae4be9974026c5f8d346f\"\n",
        "payload = {'api_key': api_key, 'query':'How+to+learn+Python'}\n",
        "resp = requests.get('https://api.scrapingdog.com/google', params=payload)\n",
        "data = resp.json()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ebiaITB87Dh"
      },
      "outputs": [],
      "source": [
        "# gimini\n",
        "# AIzaSyCE2CQCj5E8Ozbau9uwdTwJFyoNLXu0pMs\n",
        "# self.capsolver_key = api_key or \"CAP-BD48765631E316FCA364D5F2F776E224\"\n",
        "#         self.twocaptcha_key = \"be2f60c987f4a663ae7174f01124a955\"  # Fallback\n",
        "#         self.anticaptcha_key = \"b5e105fc196e48fe8286073c302eb153\"\n",
        "# https://api.webscrapingapi.com/v1?api_key=bzJh1WiHYl14pC1ndXHFjIMqO8MTaZmG\n",
        "# https://app.zenscrape.com/api/v1/get?apikey=5868e050-d37c-11eb-8473-6153a26b8aed\n",
        "# api_key_scrapingant_com = \"6ae0de59fad34337b2ee86814857278a\"\n",
        "\n",
        "# iproyal\n",
        "# de1.4g.iproyal.com:7281:uRV5sGr:mC94nHLam2AuRgw\n",
        "# dgQk19rhe9IfT5dQ:HbzWVsg8EIRrQsyj@geo.iproyal.com:12321\n",
        "# qCVIVja9MOwhXLdq:agcOHyRhV4b6DWCn_country-us_city-tallahassee@geo.iproyal.com:12321"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Optional\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# -----------------------\n",
        "# Configuration\n",
        "# -----------------------\n",
        "API_BASE = \"http://api.webscrapingapi.com/v1\"  # or https://api.webscrapingapi.com/v1\n",
        "\n",
        "TARGET_BASE = \"http://books.toscrape.com/\"\n",
        "CATALOGUE_FIRST = TARGET_BASE + \"catalogue/page-1.html\"\n",
        "\n",
        "# Controls\n",
        "REQUEST_TIMEOUT = 30\n",
        "MAX_PAGES = 5  # safety cap for demo; set higher if you need more\n",
        "RETRY_LIMIT = 3\n",
        "RETRY_BACKOFF = 2  # seconds, multiplied each retry\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Helpers: request via WebScrapingAPI\n",
        "# -----------------------\n",
        "def fetch_via_webscrapingapi(url, api_key, extra_params) -> str:\n",
        "    \"\"\"\n",
        "    Fetch `url` using api.webscrapingapi.com and return response.text.\n",
        "    Adjust query params as needed (render_js, browser, proxy_type, country_code, etc.)\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"api_key\": api_key,\n",
        "        \"url\": url,\n",
        "        # \"render_js\": \"true\",   # enable if the page needs JS rendering (slower / costs more)\n",
        "        # \"proxy_type\": \"residential\",  # example\n",
        "        # \"country_code\": \"us\",  # example\n",
        "    }\n",
        "    if extra_params:\n",
        "        params.update(extra_params)\n",
        "\n",
        "    for attempt in range(1, RETRY_LIMIT + 1):\n",
        "        try:\n",
        "            resp = requests.get(API_BASE, params=params, timeout=REQUEST_TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "            # WebScrapingAPI typically returns the target page HTML as the response body\n",
        "            return resp.text\n",
        "        except requests.HTTPError as e:\n",
        "            logging.warning(\"HTTP error fetching %s: %s (attempt %d)\", url, e, attempt)\n",
        "        except requests.RequestException as e:\n",
        "            logging.warning(\"Request error fetching %s: %s (attempt %d)\", url, e, attempt)\n",
        "\n",
        "        if attempt < RETRY_LIMIT:\n",
        "            backoff = RETRY_BACKOFF ** attempt\n",
        "            logging.info(\"Retrying in %d seconds...\", backoff)\n",
        "            time.sleep(backoff)\n",
        "    raise RuntimeError(f\"Failed to fetch {url} after {RETRY_LIMIT} attempts\")\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Parsers\n",
        "# -----------------------\n",
        "def parse_book_card(card) -> Dict:\n",
        "    \"\"\"\n",
        "    Given a BeautifulSoup element for a book card, extract info.\n",
        "    \"\"\"\n",
        "    # Title\n",
        "    title_el = card.select_one(\"h3 a\")\n",
        "    title = title_el[\"title\"].strip() if title_el and title_el.has_attr(\"title\") else title_el.get_text(strip=True)\n",
        "\n",
        "    # Link (relative)\n",
        "    rel_link = title_el[\"href\"]\n",
        "    # links are like \"../../../the-book_123/index.html\" or \"a/b.html\" — normalize to absolute\n",
        "    product_url = requests.compat.urljoin(TARGET_BASE + \"catalogue/\", rel_link)\n",
        "\n",
        "    # Price\n",
        "    price_el = card.select_one(\".product_price .price_color\")\n",
        "    price = price_el.get_text(strip=True) if price_el else None\n",
        "\n",
        "    # Availability\n",
        "    avail_el = card.select_one(\".product_price .availability\")\n",
        "    availability = avail_el.get_text(strip=True) if avail_el else None\n",
        "\n",
        "    # Rating (from class)\n",
        "    rating_el = card.select_one(\".product_pod .star-rating\")\n",
        "    rating = None\n",
        "    if rating_el:\n",
        "        classes = rating_el.get(\"class\", [])\n",
        "        # classes example: [\"star-rating\", \"Three\"]\n",
        "        for c in classes:\n",
        "            if c.lower() != \"star-rating\":\n",
        "                rating = c  # e.g., \"Three\"\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"product_url\": product_url,\n",
        "        \"price\": price,\n",
        "        \"availability\": availability,\n",
        "        \"rating\": rating,\n",
        "    }\n",
        "\n",
        "\n",
        "def parse_books_from_listing(html: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parse a listing page and return list of book dicts.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    cards = soup.select(\"article.product_pod\")\n",
        "    books = [parse_book_card(card) for card in cards]\n",
        "    return books\n",
        "\n",
        "\n",
        "def find_next_page_url(html: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Return absolute URL for the 'next' page in pagination, or None if there isn't one.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    next_li = soup.select_one(\"li.next a\")\n",
        "    if not next_li:\n",
        "        return None\n",
        "    rel = next_li.get(\"href\")\n",
        "    # On books.toscrape, pagination links are relative to the catalogue directory\n",
        "    next_url = requests.compat.urljoin(TARGET_BASE + \"catalogue/\", rel)\n",
        "    return next_url\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# High-level scraping flow\n",
        "# -----------------------\n",
        "def scrape_books(start_url, max_pages, api_key) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Crawl listing pages starting at start_url and collect book items.\n",
        "    \"\"\"\n",
        "    all_books = []\n",
        "    url = start_url\n",
        "    page_count = 0\n",
        "\n",
        "    while url and page_count < max_pages:\n",
        "        logging.info(\"Fetching page %d: %s\", page_count + 1, url)\n",
        "        html = fetch_via_webscrapingapi(url, api_key, extra_params=None)\n",
        "        books = parse_books_from_listing(html)\n",
        "        logging.info(\"Found %d books on page %d\", len(books), page_count + 1)\n",
        "        all_books.extend(books)\n",
        "\n",
        "        next_url = find_next_page_url(html)\n",
        "        if not next_url:\n",
        "            logging.info(\"No next page found; finished.\")\n",
        "            break\n",
        "\n",
        "        url = next_url\n",
        "        page_count += 1\n",
        "        time.sleep(1)  # polite pause; WebScrapingAPI already handles many networking concerns\n",
        "\n",
        "    return all_books\n"
      ],
      "metadata": {
        "id": "e91ieq_dd-tY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Quick demo: scrape first 3 pages\n",
        "    try:\n",
        "        results = scrape_books(start_url=\"http://books.toscrape.com/\", max_pages=1, api_key=\"b948b414-dd1d-4d98-8688-67f154a74fe8\")\n",
        "        print(f\"Total books scraped: {len(results)}\\nSample:\")\n",
        "        for i, b in enumerate(results[:10], 1):\n",
        "            print(f\"{i}. {b['title']} — {b['price']} — {b['rating']} — {b['availability']}\\n   {b['product_url']}\")\n",
        "    except Exception as exc:\n",
        "        logging.exception(\"Scraping failed: %s\", exc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kWu2RoXfrWC",
        "outputId": "5cabe599-a1da-49fe-f2e6-cc7832cd1685"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:HTTP error fetching http://books.toscrape.com/: 401 Client Error: Unauthorized for url: https://api.webscrapingapi.com/v1?api_key=b948b414-dd1d-4d98-8688-67f154a74fe8&url=http%3A%2F%2Fbooks.toscrape.com%2F (attempt 1)\n",
            "WARNING:root:HTTP error fetching http://books.toscrape.com/: 401 Client Error: Unauthorized for url: https://api.webscrapingapi.com/v1?api_key=b948b414-dd1d-4d98-8688-67f154a74fe8&url=http%3A%2F%2Fbooks.toscrape.com%2F (attempt 2)\n",
            "WARNING:root:HTTP error fetching http://books.toscrape.com/: 401 Client Error: Unauthorized for url: https://api.webscrapingapi.com/v1?api_key=b948b414-dd1d-4d98-8688-67f154a74fe8&url=http%3A%2F%2Fbooks.toscrape.com%2F (attempt 3)\n",
            "ERROR:root:Scraping failed: Failed to fetch http://books.toscrape.com/ after 3 attempts\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-250705893.py\", line 4, in <cell line: 0>\n",
            "    results = scrape_books(start_url=\"http://books.toscrape.com/\", max_pages=1, api_key=\"b948b414-dd1d-4d98-8688-67f154a74fe8\")\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3353545889.py\", line 142, in scrape_books\n",
            "    html = fetch_via_webscrapingapi(url, api_key, extra_params=None)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3353545889.py\", line 58, in fetch_via_webscrapingapi\n",
            "    raise RuntimeError(f\"Failed to fetch {url} after {RETRY_LIMIT} attempts\")\n",
            "RuntimeError: Failed to fetch http://books.toscrape.com/ after 3 attempts\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUAoDeeIh41dmyct093Jp7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}